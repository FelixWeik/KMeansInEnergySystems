\chapter{Background}
\label{cha:background}
% existing methods and concepts are fully introduced
% Mathematics of the methods is correctly covered
% self-developed/chosen solution method is motivated and described fully

% Self-developed / chosen solution method is motivated and described fully
Summarize what the paper aims to do.
How will the following methodological concepts help to achieve the goal of the paper?
Why exactly are these concepts chosen?
What keywords were used to find the papers?
What were the search results?
How will the papers help in answering the research questions?


% Was haben sie in den Papers der Research Questions benutzt?


% Viel aus dem Paper über die Initialisierungsmethoden zitieren, da stehen viele tolle paper drin

% Was möchte ich in diesem Abschnitt vermitteln?
% - Vorstellung von K-Means:
%   - Vorstellung eines generellen Problems
%   - Funktionsweiße mit mathematischen Grundlagen
%   - Die Elbow-Methode
%   - Initialisierungsmethoden
%   - Stärken und Schwächen
%

\section{K-Means Clustering}
\label{sec:k_means_clustering}
% General definition/idea 
Organizing unlabeled data \cite{EZU-CPF} is the general problem of the clustering analysis.
The meaningful grouping of such unlabeled data is regarded as data clustering \cite{ABI-RKC}.
This allows for the identification of patterns and trends in the data, which can be used for further analysis, such as applying more advanced theories and methods.

% Introduce the general problem the algorithm tries to solve
K-means is a partitional clustering algorithm \cite{SIN-UKC}.
It is used to group a set of n data points from z dimensions into k clusters.
This means, that a single partition of the initial dataset is produced with each point being assigned to a distinct cluster \cite{SIN-UKC}.
Clusters are produced heuristically while optimizing a criterion function defined globally on all data objects or locally on the subset of the data objects \cite{ZHU-EPC}.

\subsection{Mathematical Concepts}
As mentioned before, k-means divides a set of n data points from z dimensions into k clusters.
This is done by minimizing the sum of squared distances between a datapoint and its assigned cluster center within the whole cluster \cite{HAR-KMA}.
Doing this globally is an NP-hard problem.
Therefore, the algorithm seeks local optima, such that no point can be assigned to a different cluster and the result converges \cite{SEL-GCT}, \cite{HAR-KMA}.

The initial cluster centers are predefined as explained in \autoref{subsec:initialisation_methods}.
Next, the means of the initial clusters are calculated.
Then, each data point is assigned to the cluster with the closest mean.
These steps are repeated until the cluster centers do not change anymore or the square sum of errors stays the same for multiple iterations \cite{HAR-KMA}.

\paragraph*{Initialisation Methods:}
\label{subsec:initialisation_methods}
The purpose of the initialization methods is to find the initial cluster centers.
There are multiple methods to do so, for example, \texttt{RANDOM} \cite{PEN-ECI}, the \texttt{Forgy Approach} \cite{AND-CAA}, the \texttt{Macqueen Approach} \cite{MCQ-MCA}, and the \texttt{Kaufman Approach} \cite{KAU-FGD}.

The \texttt{RANDOM} is the most commonly used method due to its simplicity while still being an effective initialization method in terms of convergence speed and robustness \cite{PEN-ECI}.
Therefore it is assumed to be the default initialization method in this thesis.

\paragraph*{Pseudocode}
\begin{enumerate}
    \item The algorithm requires an input matrix of n data points in z dimensions and the initial cluster centers as k points in z dimensions \cite{HAR-KMA}.
          The initial cluster centers are chosen according to the used initialization method as explained in \autoref{subsec:initialisation_methods}.
    \item The average of each cluster is calculated by using $C_i = (1/M) \sum_{j=1}^{M}x_j$ with $C_i$ being the average of cluster $i$, $M$ the number of points in cluster $i$, and $x_j$ the $j$-th point in cluster $i$ \cite{SYA-IKC}.
    \item Iterate over all data points assigning each point to the nearest cluster center.
          To calculate the distance, the euclidean distance $d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ is used.
    \item Steps 2 and 3 are repeated until the criterion function converges.
          The criterion function is \begin{equation}\label{eq:sse}E=\sum_{i=1}^{k} \sum_{P \in C_i}|p-m_i|\end{equation} with $E$ being the square error-sum, $p$ the point in space, and $m_i$ the average of cluster $C_i$ \cite{LIU-BDE}.
\end{enumerate}

\subsection{Finding k: The Elbow Method}
The elbow method is a heuristic to find the optimal number $k$ of clusters for a given problem.
It is calculated by plotting the square sum of errors (\autoref{eq:sse}) against the number of clusters.
The optimal number of clusters is the point where the graph's slope changes the most, which is the so-called elbow \cite{SYA-IKC}.

\subsection{Strengths and Weaknesses}
The actual strengths and weaknesses of the algorithm will be discussed in \autoref{cha:conclusion}.

\paragraph*{Strengths:}

\paragraph*{Weaknesses:}
Due to the algorithm's greedy nature, the algorithm may converge to a local minimum \cite{JAI-DCB}.
Therefore, multiple runs for a given \texttt{k} value with different initial cluster centers are needed to obtain the optimal cluster result \cite{EZU-CPF}, \cite{BAR-LVG}.

% Was sollen die folgenden beiden sections vermitteln?
%  Wichtig: Vergleichen zwischen verschiedenen Methoden, aber nur bei großen Unterschieden.
%  - Grundlegende Vorstellung der Methoden der Papers
%   - Aus welcher Perspektive gehen die Paper an die jeweilige Fragestellung heran?
%   - Warum verwendet das jeweilige Paper diese Methoden (wie sinnvoll ist das im gegebenen Kontext)? 
%   - Was möchten die jeweiligen Paper erreichen?
%   - In welchem Kontext stehen die Paper (bezogen auf die research questions)?
%   - Wie wenden die Paper k-means an?
%   - Dabei nicht jede Methodik einzeln aufschlüsseln, eher zusammenfassen.
%   - Werden die Daten danach weiter verarbeitet (weiterführende Theorien, ...)?
\section{Methodology of the Research Questions}
As presented in \autoref{cha:introduction}, two research questions are addressed in this thesis.
Both questions are answered by research papers answering similar questions applying k-means clustering.
The following sections will introduce the methodologies of the papers for each research question, comparing and discussing them.

\subsection{K-Means in the Context of Energy-Saving Incentives}
To answer the first research question, two papers are introduced.
Each paper has a different perspective on the given question, namely the perspective of the industry and the perspective of private households.
\textit{Big data-informed energy efficiency assessment of China industry sectors based on K-means clustering} \cite{LIU-BDE} takes the perspective of the industry.
Using k-means clustering, the paper compares different industry sectors and single companies within these sectors to find high-, medium-, and low-consumption clusters according to different environmental metrics.
\textit{Identifying Home System of Practices for Energy Use with K-Means Clustering Techniques} \cite{MAL-HBP} takes the perspective of private households.
Applying social theory to the results of k-means clustering, the paper identifies different home systems of best practices, explaining how habits, behaviors, and socially shared practices exhibit variation in the way households consume energy.

Both papers differ only slightly in their methodology.
\cite{LIU-BDE} applies k-means to cluster different industry sectors and single companies into high, medium, and low energy efficiency.
Therefore, the value for $k$ is predefined as three.
The used data is the list of all companies trading A shares on the Shanghai (SSE, \url{http://www/sse.com.cn}) and Shenzhen stock exchanges (SZSE, \url{http://www.szse.cn}) since these companies are required to disclose their environmental information every year.
This includes the timespan from 2000 to 2015.
Since the data is noisy, a cleanup of the data is necessary.
After applying k-means, no more theories are applied.
\cite{MAL-HBP} applies k-means to find home systems of best practices (HSOP).
Each cluster found represents a different HSOP.
Therefore, the value for $k$ needs to be adjusted according to each dataset.
Clustering is performed on the data of the Fairwater Living Lab, which delivers data from 39 Australian households from July 1st, 2019 to March 31st, 2021. 
The data is collected by the researchers and includes no noise, therefore no cleanup before applying k-means is necessary.
After applying k-means, the results are further analyzed using social theories and polling the households' residents.

%//TODO: Werden die Daten normalisiert? => evenetuelle Rückschlüsse auf Methodik von Jessen

\subsection{K-Means in the Context of Natural Disaster- and Electrical Fault-Impacted Load Profiles}
One main paper with several sub-papers is introduced to answer the second research question.
\textit{Identification of Natural Disaster Impacted Electricity Load Profiles with k-means Clustering Algorithm} \cite{JES-IND} investigates load profile characteristics and identifies natural disaster impacted load profiles based on k-means clustering for Lombok, Indonesia.
Further aims include the identification of post-natural disaster days capturing the characteristics of natural disaster events.
This helps to understand the impact of natural disasters based on the electricity load profiles.

The paper's methodology differs in the application of the clusters and the preprocessing.
First, several datasets are collected: The electricity load profiles from 2015 to 2022, and the data on the natural disasters in the same time frame.
The data on natural disasters is provided by the National Disaster Management Agency (BNPB) of Indonesia \cite{BNP-CAD}.
Then, the electricity load profile data is scaled by applying the MinMaxScaler \cite{SKL-MMS}.
For each value $x$ in the dataset is scaled applying \begin{equation}\label{eq:minmaxscaler} 
      x_{transformed} = \frac{x - x_{min}}{x_{max} - x_{min}}
\end{equation} for each of the datapoints \cite{JOJ-ENP}.

After preprocessing, the elbow method and the k-means clustering algorithm are applied as explained in \autoref{sec:k_means_clustering}.
The clustering is executed on the whole dataset and the year-by-year data.
Each cluster is then labeled as normal or abnormal with abnormal clusters being natural disasters or electrical fault impacted.
This happens by applying the data on natural disasters and checking whether a natural disaster occurred in the given cluster.

% Was ist wichtig in diesem Absatz bzw. was unterscheidet diese Methodik von denen davor?
% - Einführung in die gesammelte Datenmenge => wir haben jetzt zwei Datensätze, auf einem wird geclustert, auf dem anderen werden die Cluster danach analysiert
% - Die Daten werden mit dem MinMaxScaler skaliert
% - Cluster werden mit den Daten zu den Naturkatastrophen gelabeled => Darauf werden dann die einzelnen Naturkatastrophen auf die Climate Resilience des Systems analysisert


% % Data collection
% First, data from 2015 to 2022 is collected and prepared.
% The data collection happens in multiple steps: First, the electricity load data and the natural disaster data are collected.
% The data on natural disasters is provided by the National Disaster Management Agency (BNPB) of Indonesia \cite{BNP-CAD}.
% The paper focuses on earthquakes since they happen the most frequently while having a significant impact on the electricity system.
% Therefore data on the time, the Richter scale severity and the epicenter location of the earthquakes is collected.

% % Data preparation (scaling the data)
% The MinMaxScaler is applied to scale the data to a range of 0 to 1.
% It preserves the shape of the original distribution so the original information is not lost \cite{GOG-WSI}.
% % For each value $x$ in the dataset is scaled applying \begin{equation}\label{eq:minmaxscaler} 
% %       x_{transformed} = \frac{x - x_{min}}{x_{max} - x_{min}}
% % \end{equation} for each of the datapoints \cite{JOJ-ENP}.
% This is done to avoid the influence of the different scales of the data on the clustering result and therefore improves the total clustering performance \cite{GOG-WSI}.
% The optimal number of clusters is found using the elbow method as explained in \autoref{sec:k_means_clustering}.

% % Seasonal Distribution Analysis
% K-Means is executed on the entire load profile duration and a year-by-year profile duration.
% It is expected to reveal general trends and characteristics as well as precise variations in the load profile.
% After clustering each daily profile will be assigned to a distinct cluster.
% % Separation into normal and abnormal load profiles
% A normal daily electricity load profile is an electricity profile that either resembles the average daily load profile of the specific year in shape and magnitude or is a product of electricity consumer behavior variations.
% An abnormal profile is defined as either experiencing few significant power consumption drops or containing low-magnitude power consumption values that are non-consumer behavior induced.

% % Cluster Labelling
% Clusters are labeled either as disturbance-impacted or normal.
% Disturbance-impacted clusters are either natural disaster-impacted or electrical fault-impacted.
% If a natural disaster does not occur simultaneously with an abnormal load profile, the profile is labeled as electrical fault-impacted.

% % Further analysis of the data regarding climate resilience capability
% After clustering, the data is further analyzed regarding climate resilience capability.
% This means, the data is analyzed in terms of robustness, resourcefulness, and recovery of the electricity system.
% This includes data on the outbreak of the disruption, the impact size, time at the bottom, and recovery duration. and the average restoration rate of the electricity system.


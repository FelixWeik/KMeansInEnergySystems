\section{Background}
\label{cha:background}
% existing methods and concepts are fully introduced
% Mathematics of the methods is correctly covered
% self-developed/chosen solution method is motivated and described fully

% Self-developed / chosen solution method is motivated and described fully
% Summarize what the paper aims to do.
% How will the following methodological concepts help to achieve the goal of the paper?
% Why exactly are these concepts chosen?
% What keywords were used to find the papers?
% What were the search results?
% How will the papers help in answering the research questions?


% Was haben sie in den Papers der Research Questions benutzt?


% Viel aus dem Paper über die Initialisierungsmethoden zitieren, da stehen viele tolle paper drin

% Was möchte ich in diesem Abschnitt vermitteln?
% - Vorstellung von K-Means:
%   - Vorstellung eines generellen Problems
%   - Funktionsweiße mit mathematischen Grundlagen
%   - Die Elbow-Methode
%   - Initialisierungsmethoden
%   - Stärken und Schwächen

\subsection{K-Means Clustering}
\label{sec:k_means_clustering}
% General definition/idea 
Organizing unlabeled data \cite{EZU-CPF} is the general problem of the clustering analysis.
The meaningful grouping of such unlabeled data is regarded as data clustering \cite{ABI-RKC}.
This allows for the identification of patterns and trends in the data, which can be used for further analysis, such as applying more advanced theories and methods.

% Introduce the general problem the algorithm tries to solve
K-means is a partitional clustering algorithm \cite{SIN-UKC}.
It is used to group a set of n data points from z dimensions into k clusters.
This means, that a single partition of the initial dataset is produced with each point being assigned to a distinct cluster \cite{SIN-UKC}.
Clusters are produced heuristically while optimizing a criterion function defined globally on all data objects or locally on the subset of the data objects \cite{ZHU-EPC}.

\subsubsection{Mathematical Concepts}
K-Means clustering utilizes the concept of minimizing the sum of squared distances between a datapoint and its assigned cluster center within the whole cluster \cite{HAR-KMA}
Doing this globally exists, but it is computationally expensive \cite{LIS-GKC}.
Therefore, the algorithm seeks local optima, such that no point can be assigned to a different cluster and the result converges \cite{SEL-GCT}.
An execution of the general algorithm is shown in \ref{subsec:general_algorithm}.

\subsubsection{Initialisation Methods}
\label{subsec:initialisation_methods}
Initialization methods are used to find the initial cluster centers and the initial clustering.
They provide the first step in the k-means algorithm.
There are multiple methods to do so, with the most popular approaches being \texttt{RANDOM} \cite{PEN-ECI}, the \texttt{Forgy Approach} \cite{AND-CAA}, the \texttt{Macqueen Approach} \cite{MCQ-MCA}, and the \texttt{Kaufman Approach} \cite{KAU-FGD}.

The \texttt{RANDOM} approach is the most commonly used method.
It is provided with the datasets and a value for $k$ and then randomly selects $k$ points from the dataset as the initial cluster centers.
Iterating over the dataset, each point is assigned to a random cluster center.
\texttt{RANDOM} is very popular due to it being an effective initialization method in terms of convergence speed and robustness while being fast in execution \cite{PEN-ECI}.
Therefore it is assumed to be the default initialization method in this thesis.

Other initialization methods differ in the way they choose the initial cluster centers or assign the initial clusters.
\texttt{FORGY} provides a similar approach to \texttt{RANDOM}, but the points are initially assigned to the nearest randomly chosen cluster center.
The usual distance metric is the Euclidean distance \cite{AND-CAA}.
Despite this being a seemingly better first step, the \texttt{RANDOM} approach has similar results in practice in terms of convergence speed while being less computationally expensive \cite{AND-CAA}.

\subsubsection{General Algorithm}
\label{subsec:general_algorithm}
K-means can be implemented in multiple variants.
The most general implementation using the Euclidean distance is shown in the following.
\begin{enumerate}
    \item The algorithm requires an input matrix of n data points in z dimensions and the initial cluster centers as k points in z dimensions \cite{HAR-KMA}.
          The initial cluster centers and the initial clustering are chosen according to the used initialization method as explained in \ref{subsec:initialisation_methods}.
          This review assumes the \texttt{RANDOM} approach as the default initialization method.
    \item The average of each cluster is calculated by using $C_i = \frac{1}{M} \sum_{j=1}^{M}x_j$ with $C_i$ being the average of cluster $i$, $M$ the number of points in cluster $i$, and $x_j$ the $j$-th point in cluster $i$ \cite{SYA-IKC}.
          $C_i$ is then used as the new cluster center of cluster $i$.
    \item Iterate over all data points assigning each point to the nearest cluster center.
          To calculate the distance, the euclidean distance $d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ is used.
    \item Steps 2 and 3 are repeated until the criterion function converges.
          The criterion function (Equation \ref{eq:sse}) represents the sum of square errors.
          It is defined as \begin{equation}\label{eq:sse}E=\sum_{i=1}^{k} \sum_{P \in C_i}|p-m_i|\end{equation} with $p$ being the point in space, and $m_i$ being the average of cluster $C_i$ \cite{LIU-BDE}.
\end{enumerate}

\subsubsection{Finding k: The Elbow Method}
The elbow method is a heuristic to find the optimal number $k$ of clusters for a given problem \cite{SYA-IKC}.
It is calculated by plotting the square sum of errors (\ref{eq:sse}) against the number of clusters.
The optimal number of clusters is the point where the graph's slope changes the most, the so-called elbow \cite{SYA-IKC}.

\subsubsection{Data Scaling: The MinMaxScaler}
The MinMaxScaler is a data scaling tool of the \texttt{Scikit Learn} library \cite{SKL-MMS}.
It scales given data to a range of 0 to 1 and keeps the shape of the original distribution.
Most of the following papers use this scaler before applying k-means clustering.
For each value $x$ in the dataset is scaled applying \begin{equation}\label{eq:minmaxscaler} 
      x_{transformed} = \frac{x - x_{min}}{x_{max} - x_{min}}
\end{equation} for each of the datapoints \cite{JOJ-ENP}.
This ensures that the different scales of the data do not influence the clustering result and therefore improves the total clustering performance \cite{GOG-WSI}.

% Was sollen die folgenden beiden sections vermitteln?
%  Wichtig: Vergleichen zwischen verschiedenen Methoden, aber nur bei großen Unterschieden.
%  - Grundlegende Vorstellung der Methoden der Papers
%   - Aus welcher Perspektive gehen die Paper an die jeweilige Fragestellung heran?
%   - Warum verwendet das jeweilige Paper diese Methoden (wie sinnvoll ist das im gegebenen Kontext)? 
%   - Was möchten die jeweiligen Paper erreichen?
%   - In welchem Kontext stehen die Paper (bezogen auf die research questions)?
%   - Wie wenden die Paper k-means an?
%   - Dabei nicht jede Methodik einzeln aufschlüsseln, eher zusammenfassen.
%   - Werden die Daten danach weiter verarbeitet (weiterführende Theorien, ...)?
\subsection{Methodology of the Research Questions}
\label{sec:methodology_of_the_research_questions}
Three papers provide findings impacting the given research questions.
Each paper has a different perspective, namely the perspective of the industry, private households, and energy load profiles of a specific country.
The following section describes the goals of each paper and how the research is conducted to achieve these goals.

\enquote{\textit{Big Data-Informed Energy Efficiency Assessment of China Industry Sectors based on K-Means Clustering}} by Liu et al. \cite{LIU-BDE} focuses on clustering Chinese industry sectors and distinct companies into three clusters, based on environmental perfomance derived metrics.
These three clusters are then used to identify and compare the most energy-intensive sectors and companies in big data masses.
Liu et al. apply k-means clustering on disclosed environmental performance data of 15.128 Chinese companies between 2000 and 2015.
By applying k-means clustering with a fixed value of $k=3$, the companies and industry sectors are divided into low-, mid-, and high-performing clusters according to the chosen metrics, for example, $SO_2$ emissions.

\enquote{\textit{Identifying Home System of Practices for Energy Use with K-Means Clustering}} by Malatesta et al. \cite{MAL-HBP} focuses on finding home systems of practices (HSOPs) by clustering energy load profiles of 39 private households in Blacktown, Australia.
Each cluster represents a home system of practices (HSOP), a habit or behavior that alters the standard load profile.
The clustering is executed on the total dataset and separately for each household, each cluster is then labeled as a distinct HSOP.
This allows the researchers to identify general trends from the total data clustering and precise comparisons between households by comparing the individual household clustering results.
By applying social theory and survey results distributed to the households' residents afterward it can be derived how behaviors and habits impact electricity consumption.

\enquote{\textit{Identification of Natural Disaster Impacted Electricity Laod Profiles with k-means Clustering Algorithm}} by Jessen et al. \cite{JES-IND} investigates the load profiles of Lombok, Indonesia between 2015 and 2022.
The data is clustered in a varying number of clusters.
The value of $k$ is chosen according to the given dataset using the elbow method.
These clusters are then algorithmically labeled as normal or abnormal, with abnormal being either natural disaster-impacted or electrical fault-impacted which means non-human-induced.
This shows how k-means helps in understanding patterns, trends and the consequences of different impacts on the electricity load profiles.

All of these papers contribute to understanding general electricity load profiles and how different impacts alter the consumption data.
The results are then used to conclude how the design of energy-saving incentives can be improved.
